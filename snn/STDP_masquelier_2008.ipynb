{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STDP Finds the Start of Repeating Patterns in Continuous Spike Trains\n",
    "\n",
    "In this notebook we will reproduce the experiments described in [Masquelier & Thorpe (2008)](https://www.semanticscholar.org/paper/Spike-Timing-Dependent-Plasticity-Finds-the-Start-Masquelier-Guyonneau/432b5bfa6fc260289fef45544a43ebcd8892915e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports will be used in the notebook\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =(12,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIF neuron model\n",
    "\n",
    "The LIF neuron model used in this experiment is based on Gerstner's [Spike Response Model](http://lcn.epfl.ch/~gerstner/SPNM/node26.html#SECTION02311000000000000000).\n",
    "\n",
    "At every time-step, the neuron membrane potential p is given by the formula:\n",
    "\n",
    "$$p=\\eta(t-t_{i})\\sum_{j|t_{j}>t_{i}}{}w_{j}\\varepsilon(t-t_{j})$$\n",
    "\n",
    "where $\\eta(t-t_{i})$ is the membrane response after a spike at time $t_{i}$:\n",
    "\n",
    "$$\\eta(t-t_{i})=K_{1}exp(-\\frac{t-t_{i}}{\\tau_{m}})-K_{2}(exp(-\\frac{t-t_{i}}{\\tau_{m}})-exp(-\\frac{t-t_{i}}{\\tau_{s}}))$$\n",
    "\n",
    "and $\\varepsilon(t)$ describes the Excitatory Post-Synaptic Potential of each synapse spike at time $t_{j}$:\n",
    "\n",
    "$$\\varepsilon(t-t_{j})=K(exp(-\\frac{t-t_{j}}{\\tau_{m}})-exp(-\\frac{t-t_{j}}{\\tau_{s}}))$$\n",
    "\n",
    "Note that K has to be chosen so that the max of $\\eta(t)$ is 1, knowing that $\\eta(t)$ is maximum when:\n",
    "$$t=\\frac{\\tau_{m}\\tau_{s}}{\\tau_{m}-\\tau_{s}}ln(\\frac{\\tau_{m}}{\\tau_{s}})$$\n",
    "\n",
    "In this simplified version of the neuron, the synaptic weights $w_{j}$ remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=None,\n",
    "                 K=2.1, K1=2.0, K2=4.0):\n",
    "\n",
    "        # Model parameters\n",
    "\n",
    "        # Membrane resting potential\n",
    "        self.p_rest = p_rest\n",
    "        \n",
    "        # Duration of the recovery period\n",
    "        self.tau_rest = tau_rest\n",
    "        \n",
    "        # Membrane time constant\n",
    "        self.tau_m = tau_m\n",
    "        \n",
    "        # Synaptic time constant\n",
    "        self.tau_s = tau_s\n",
    "        \n",
    "        # Spiking threshold\n",
    "        if T is None:\n",
    "            self.T = n_syn/4\n",
    "        else:\n",
    "            self.T = T\n",
    "        \n",
    "        # Model constants\n",
    "        self.K = K\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "\n",
    "        # The number of synapses\n",
    "        self.n_syn = n_syn\n",
    "        \n",
    "        # The synapse efficacy weights\n",
    "        self.w = tf.Variable(W)\n",
    "        \n",
    "        # The incoming spike times memory window\n",
    "        if max_spikes is None:\n",
    "            self.max_spikes = 70\n",
    "        else:\n",
    "            self.max_spikes = max_spikes\n",
    "\n",
    "        # Placeholders (ie things that are fed to the graph at runtime)\n",
    "\n",
    "        # A boolean tensor indicating which synapses have spiked during dt\n",
    "        self.new_spikes = tf.placeholder(shape=[m], dtype=tf.bool, name='new_spikes')\n",
    "\n",
    "        # The time increment since the last update\n",
    "        self.dt = tf.placeholder(dtype=tf.float32, name='dt')\n",
    "        \n",
    "        # Variables (ie things that are modified by the graph at runtime)\n",
    "\n",
    "        # The neuron memory of incoming spike times\n",
    "        self.t_spikes = tf.Variable(tf.constant(100000.0, shape=[self.max_spikes, self.n_syn]), dtype=tf.float32)\n",
    "        \n",
    "        # The last spike time insertion index\n",
    "        self.t_spikes_idx = tf.Variable(self.n_syn - 1, dtype=tf.int32)\n",
    "\n",
    "        # The relative time since the last spike (assume it was a very long time ago)\n",
    "        self.last_spike = tf.Variable(1000.0, dtype=tf.float32, name='last_spike')\n",
    "        \n",
    "        # The membrane potential\n",
    "        self.p = tf.Variable(self.p_rest,dtype=tf.float32, name='p')\n",
    "        \n",
    "        # The duration remaining in the resting period (between 0 and self.tau_s)\n",
    "        self.t_rest = tf.Variable(0.0,dtype=tf.float32, name='t_rest')\n",
    "\n",
    "    # Excitatory post-synaptic potential (EPSP)\n",
    "    def epsilon_op(self):\n",
    "\n",
    "        # We only use the negative value of the relative spike times\n",
    "        spikes_t_op = tf.negative(self.t_spikes)\n",
    "\n",
    "        return self.K *(tf.exp(spikes_t_op/self.tau_m) - tf.exp(spikes_t_op/self.tau_s))\n",
    "    \n",
    "    # Membrane spike response\n",
    "    def eta_op(self):\n",
    "        \n",
    "        # We only use the negative value of the relative time\n",
    "        t_op = tf.negative(self.last_spike)\n",
    "        \n",
    "        # Evaluate the spiking positive pulse\n",
    "        pos_pulse_op = self.K1 * tf.exp(t_op/self.tau_m)\n",
    "        \n",
    "        # Evaluate the negative spike after-potential\n",
    "        neg_after_op = self.K2 * (tf.exp(t_op/self.tau_m) - tf.exp(t_op/self.tau_s))\n",
    "\n",
    "        # Evaluate the new post synaptic membrane potential\n",
    "        return self.T * (pos_pulse_op - neg_after_op)\n",
    "    \n",
    "    # Neuron behaviour during integrating phase (t_rest = 0)\n",
    "    def integrating_p_op(self):\n",
    "        \n",
    "        # Evaluate synaptic EPSPs. We ignore synaptic spikes older than the last neuron spike\n",
    "        epsilons_op = tf.where(tf.logical_and(self.t_spikes >=0, self.t_spikes < self.last_spike),\n",
    "                               self.epsilon_op(),\n",
    "                               self.t_spikes*0.0)\n",
    "                          \n",
    "        # Update the membrane potential with spike membrane response and weighted incoming EPSPs \n",
    "        return self.eta_op() + tf.reduce_sum(self.w * epsilons_op)\n",
    "\n",
    "    # Neuron behaviour during resting phase (t_rest > 0)\n",
    "    def resting_p_op(self):\n",
    "   \n",
    "        # Membrane potential is only impacted by the last post-synaptic spike (ignore EPSPs)\n",
    "        return self.eta_op()\n",
    "    \n",
    "    def update_spikes_times(self):\n",
    "        \n",
    "        # Increase the age of all the existing spikes by dt\n",
    "        old_spikes_op = self.t_spikes.assign_add(tf.ones(tf.shape(self.t_spikes), dtype=tf.float32) * self.dt)\n",
    "\n",
    "        # Increment last spike index (modulo max_spikes)\n",
    "        new_idx_op = self.t_spikes_idx.assign(tf.mod(self.t_spikes_idx + 1, self.max_spikes))\n",
    "\n",
    "        # Create a list of coordinates to insert the new spikes\n",
    "        idx_op = tf.constant(1, shape=[self.n_syn], dtype=tf.int32) * new_idx_op\n",
    "        coord_op = tf.stack([idx_op, tf.range(self.n_syn)], axis=1)\n",
    "\n",
    "        # Create a vector of new spike times (non-spikes are assigned a very high time)\n",
    "        new_spikes_op = tf.where(self.new_spikes,\n",
    "                                 tf.constant(0.0, shape=[self.n_syn]),\n",
    "                                 tf.constant(100000.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Replace older spikes by new ones\n",
    "        return tf.scatter_nd_update(old_spikes_op, coord_op, new_spikes_op)\n",
    "    \n",
    "    def integrating_w_op(self):\n",
    "        \n",
    "        # For the base LIF Neuron, the weights remain constants when integrating\n",
    "        return tf.identity(self.w)\n",
    "\n",
    "    def firing_w_op(self):\n",
    "\n",
    "        # For the base LIF Neuron, the weights remain constants when firing\n",
    "        return tf.identity(self.w)\n",
    "    \n",
    "    def response(self):\n",
    "        \n",
    "        # Update our internal memory of the synapse spikes (age older spike, add new ones)\n",
    "        update_spikes_op = self.update_spikes_times()\n",
    "        \n",
    "        # Increase the relative time of the last spike by the time elapsed\n",
    "        last_spike_age_op = self.last_spike.assign_add(self.dt)\n",
    "        \n",
    "        # Evaluate the new membrane potential, making sure the synapse spikes and last spike time are updated first\n",
    "        with tf.control_dependencies([update_spikes_op, last_spike_age_op]):\n",
    "            p_op = tf.cond(self.t_rest > 0.0,\n",
    "                           self.resting_p_op,     # The neuron is resting, and synaptic input is ignored \n",
    "                           self.integrating_p_op) # Integrate both synaptic input and spike dynamics\n",
    "        \n",
    "        # Update weights\n",
    "        w_op = tf.cond(p_op > self.T,\n",
    "                       self.firing_w_op,      # The neuron is firing\n",
    "                       self.integrating_w_op) # Normal behavior\n",
    "\n",
    "        # Update the time of the last spike, but only once the weights have been updated\n",
    "        with tf.control_dependencies([w_op]):\n",
    "            last_spike_op = tf.cond(p_op > self.T,\n",
    "                                    lambda: self.last_spike.assign(0.0),  # The neuron is firing, the last spike is now\n",
    "                                    lambda: tf.identity(self.last_spike)) # Nothing to do\n",
    "        # Update the resting period\n",
    "        t_rest_op = tf.cond(p_op > self.T,\n",
    "                            lambda: self.t_rest.assign(self.tau_rest),    # The neuron is firing, start resting period\n",
    "                            lambda: self.t_rest.assign(tf.maximum(self.t_rest - self.dt, 0.0))) # Decrease resting period\n",
    "        \n",
    "        # We finally update the internal membrane potential after the resting period\n",
    "        # and last spike times have been updated\n",
    "        with tf.control_dependencies([t_rest_op, last_spike_op]):\n",
    "            return self.p.assign(p_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulate neuron with predefined synapse input\n",
    "\n",
    "We replicate the figure 3 of the original paper by stimulating a LIF neuron with six consecutive spikes.\n",
    "\n",
    "The neuron has a refractory period of 1 ms and a threshold of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test neuron response with constant synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 80\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 1\n",
    "# Spiking times\n",
    "spikes = [2.0, 23.0, 44.0, 45.0, 48.0, 61.0]\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = LIFNeuron(m,W, T=1)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        syn_has_spiked = [t in spikes]\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "for spike  in spikes:\n",
    "    plt.axvline(x=spike, color='gray', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the original paper. we see that because of the leaky nature of the neuron, the stimulating spikes have to be nearly synchronous for the threshold to be reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate neuron response with random input\n",
    "\n",
    "We feed the neuron with 2000 synapses that generate spikes at random interval with a frequency of 45 Hz.\n",
    "\n",
    "The synaptic efficacy weights are arbitrarily set to 0.475 and remain constant throughout the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with constant synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 2000\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We need to keep track of input spikes over time\n",
    "spikes = np.zeros((steps,m), dtype=np.bool)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = LIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        r = np.random.uniform(0,1, size=(m))\n",
    "        syn_has_spiked = r < f * dt\n",
    "        spikes[step,:] = syn_has_spiked\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw the neuron membrane response to the 2000 random synaptic spike trains. We can see that the neuron mostly saturates and continuously generate spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw input spikes\n",
    "real_spikes = np.argwhere(spikes > 0)\n",
    "spike_index = real_spikes[:,1] + 1\n",
    "spike_timings = real_spikes[:,0]\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, m])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('spikes')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_timings, spike_index, s=2)\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Spike Timing Dependent Plasticity\n",
    "\n",
    "We extend the LIFNeuron by allowing it to modify its synapse weights using a Spike Timing Dependent Plasticity algorithm.\n",
    "\n",
    "The STDP algorithm rewards synapses where spikes occurred immediately before a neuron spike, and inflicts penalties to the synapses where spikes occur after the neuron spike.\n",
    "\n",
    "The 'rewards' are called Long Term synaptic Potentiation (LTP), and the penalties Long Term synaptic Depression (LTD).\n",
    "\n",
    "For each synapse that spiked $\\Delta{t}$ before a neuron spike:\n",
    "\n",
    "$$\\Delta{w} = a^{+}exp(-\\frac{\\Delta{t}}{\\tau^{+}})$$\n",
    "\n",
    "For each synapse that spikes $\\Delta{t}$ after a neuron spike:\n",
    "\n",
    "$$\\Delta{w} = -a^{-}exp(-\\frac{\\Delta{t}}{\\tau^{-}})$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDPLIFNeuron(LIFNeuron):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=None,\n",
    "                 K=2.1, K1=2.0, K2=4.0,\n",
    "                 a_plus=0.03125, a_minus=0.0265625, tau_plus=16.8, tau_minus=33.7):\n",
    "        \n",
    "        # Call the parent contructor\n",
    "        super(STDPLIFNeuron, self).__init__(n_syn, W, max_spikes,\n",
    "                                            p_rest, tau_rest, tau_m, tau_s, T,\n",
    "                                            K, K1, K2)\n",
    "        \n",
    "        self.a_plus = a_plus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.a_minus = a_minus\n",
    "        self.tau_minus = tau_minus\n",
    "    \n",
    "    # Long Term synaptic Potentiation\n",
    "    def LTP_op(self):\n",
    "        \n",
    "        # Reward all spikes in our memory that happened before the new spike, but after the previous one\n",
    "        rewards_op = tf.where(self.t_spikes < self.last_spike,\n",
    "                              tf.constant(self.a_plus, shape=[self.max_spikes, self.n_syn]) * tf.exp(tf.negative(self.t_spikes/self.tau_plus)),\n",
    "                              tf.constant(0.0, shape=[self.max_spikes, self.n_syn]))                              \n",
    "        \n",
    "        # Accumulate rewards for each synapse along the history axis\n",
    "        acc_rewards_op = tf.reduce_sum(rewards_op,0)\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.add(self.w, acc_rewards_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "    \n",
    "    # Long Term synaptic Depression\n",
    "    def LTD_op(self):\n",
    "\n",
    "        # Gather all spikes corresponding to the last insertion index\n",
    "        new_spikes_op = tf.gather(self.t_spikes, self.t_spikes_idx)\n",
    "\n",
    "        # Inflict penalties, inversely exponential to the time since the last spike\n",
    "        penalties_op = tf.where(new_spikes_op <= 0.0, # Older spikes at this index have positive times\n",
    "                                tf.constant(self.a_minus, shape=[self.n_syn]) * tf.exp(tf.negative(self.last_spike/self.tau_minus)),\n",
    "                                tf.constant(0.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.subtract(self.w, penalties_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "    \n",
    "    def stdp_firing_op(self):\n",
    "        \n",
    "        # Apply long-term synaptic potentiation\n",
    "        ltp_op = self.LTP_op()\n",
    "        \n",
    "        # Refractory period starts now\n",
    "        t_rest_op = self.t_rest.assign(self.tau_rest)\n",
    "\n",
    "        # Reset last spike time\n",
    "        last_spike_op = tf.assign(self.last_spike, self.dt)\n",
    "        \n",
    "        # Explicitly mark operations not used by p_op as dependencies\n",
    "        with tf.control_dependencies([ltp_op, t_rest_op, last_spike_op]):\n",
    "            # Reset membrane potential\n",
    "            p_op = self.p.assign(self.eta_op())\n",
    "\n",
    "            return p_op\n",
    "\n",
    "    def firing_w_op(self):\n",
    "        \n",
    "        return self.LTP_op()\n",
    "\n",
    "    def integrating_w_op(self):\n",
    "        \n",
    "        # Apply long-term synaptic depression if we are still close to the last spike\n",
    "        # Note that if we unconditionally applied the LTD, the weights will slowly\n",
    "        # decrease to zero if no spike occurs.\n",
    "        return tf.cond(self.last_spike < self.tau_minus*7,\n",
    "                       self.LTD_op,\n",
    "                       lambda: tf.identity(self.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test STDP with random input\n",
    "\n",
    "We apply a random input to an STDP capable LIFNeuron with a limited number of synapses, and draw the resulting rewards (green) and penalties (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with evolving synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 100\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 20\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We need to keep track of input spikes over time\n",
    "spikes = np.zeros((steps,m), dtype=np.float32)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    w_prev = W\n",
    "    delta_weights = np.zeros((steps, m))\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        r = np.random.uniform(0,1, size=(m))\n",
    "        syn_has_spiked = r < f * dt\n",
    "        spikes[step,:] = syn_has_spiked\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))\n",
    "        w_next = neuron.w.eval()\n",
    "        delta_weights[step,:] = w_next - w_prev\n",
    "        w_prev = w_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(12,9)\n",
    "# Draw input spikes\n",
    "real_spikes = np.argwhere(spikes > 0)\n",
    "spike_index = real_spikes[:,1] + 1\n",
    "spike_timings = real_spikes[:,0]\n",
    "rewards = np.argwhere(delta_weights > 0)\n",
    "rewards_timings = rewards[:,0]\n",
    "rewards_index = rewards[:,1] + 1\n",
    "penalties = np.argwhere(delta_weights < 0)\n",
    "penalties_timings = penalties[:,0]\n",
    "penalties_index = penalties[:,1] + 1\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, m])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('spikes')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_timings, spike_index, s=100)\n",
    "plt.scatter(rewards_timings, rewards_index, color='lightgreen')\n",
    "plt.scatter(penalties_timings, penalties_index, color='red')\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the graph above, we verify that the rewards (green dots) are assigned only when the neuron spike, and that they are assigned to synapses where a spike occured before the neuron spike (big blue dots).\n",
    "\n",
    "Note: a reward is assigned event if the synapse spike is not synchronous with the neuron spike, but it will be lower.\n",
    "\n",
    "We also verify that a penaly (red dot) is inflicted on every synapse where a spike occurs after a neuron spike.\n",
    "\n",
    "Note: these penalties may later be counter-balanced by a reward if a neuron spike closely follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recurrent spike trains\n",
    "\n",
    "We don't follow exactly the same procedure as in the original paper, as the evolution of the hardware and software allows us to generate spike trains more easily. The result, however, is equivalent.\n",
    "\n",
    "We generate 2000 spike trains, from which we force the 1000 first to repeat a 50 ms pattern at random intervals.\n",
    "\n",
    "We first define a random 50ms sequence, that will be used as input when the pattern is played.\n",
    "\n",
    "We then generate random spike trains at every time-step: for the whole population if we are outside the pattern, for half of it otherwise.\n",
    "\n",
    "The time to the next pattern is chosen with a probability of 0.25 among the next slices of 50 ms (omitting the first one to avoid consecutive patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with recurrent pattern\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 15000\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 2000\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "pattern_t = []\n",
    "spike_trains = np.zeros((T,m), dtype=np.bool)\n",
    "\n",
    "# First, create the 50 ms pattern\n",
    "n_syn_pattern = int(m/2)\n",
    "pattern = np.zeros((50, n_syn_pattern))\n",
    "for step in range(50):\n",
    "\n",
    "    r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "    pattern[step,:] = r < f\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    \n",
    "    syn_has_spiked = np.zeros((m), dtype=np.bool)\n",
    "\n",
    "    pat_start_time = np.random.randint(25,75)\n",
    "    pattern_t.append(pat_start_time)\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = int(step * dt)\n",
    "\n",
    "        # Evaluate the first population of neuron behavior\n",
    "        if t >= pat_start_time and t < (pat_start_time + 50):\n",
    "            # We just copy the pattern\n",
    "            syn_has_spiked[:n_syn_pattern] = pattern[t - pat_start_time]\n",
    "        else:\n",
    "            # Generate new random spikes\n",
    "            r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "            syn_has_spiked[:n_syn_pattern] = r < f * dt\n",
    "            # Evaluate the time of the next pattern\n",
    "            if t >= pat_start_time + 100:\n",
    "                pat_start_time = t\n",
    "                # We have 1/4 chances of replaying the pattern for each chunk of 50 ms\n",
    "                r = np.random.uniform(0,1)\n",
    "                while (r >= 0.25):\n",
    "                    pat_start_time += 50\n",
    "                    r = np.random.uniform(0,1)\n",
    "                pattern_t.append(pat_start_time)\n",
    "        # Evaluate the second population of neuron behavior                \n",
    "        r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "        syn_has_spiked[n_syn_pattern:] = r < f * dt\n",
    "        spike_trains[step,:] = syn_has_spiked\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(12,9)\n",
    "intervals = ([0,500],[7500, 7999],[14500,14999])\n",
    "for interval in intervals:\n",
    "    it_spike_trains = spike_trains[interval[0]:interval[1],:]\n",
    "    it_P = P[interval[0]:interval[1]]\n",
    "    it_pattern_t = np.array(pattern_t)\n",
    "    it_pattern_t = it_pattern_t[np.logical_and(it_pattern_t >=interval[0], it_pattern_t <=interval[1])]\n",
    "    # Draw input spikes, identifying the patterns\n",
    "    spikes = np.argwhere(it_spike_trains == True)\n",
    "    plt.figure()\n",
    "    plt.axis([interval[0], interval[1], 0, m])\n",
    "    plt.title('Synaptic spikes')\n",
    "    plt.ylabel('spikes')\n",
    "    plt.xlabel('Time (msec)')\n",
    "    for pat_t in it_pattern_t:\n",
    "        plt.fill_between((pat_t,pat_t+50,pat_t+50,pat_t),(0,0,m/2,m/2),facecolor='gray')\n",
    "    t, s = spikes.T\n",
    "    plt.scatter(t+interval[0], s, s=1)\n",
    "    # Draw membrane potential\n",
    "    plt.figure()\n",
    "    plt.plot(*zip(*it_P))\n",
    "    plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "    plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "    plt.title('LIF response')\n",
    "    plt.ylabel('Membrane Potential (mV)')\n",
    "    plt.xlabel('Time (msec)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
