{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STDP Finds the Start of Repeating Patterns in Continuous Spike Trains\n",
    "\n",
    "In this notebook we will reproduce the experiments described in [Masquelier & Thorpe (2008)](https://www.semanticscholar.org/paper/Spike-Timing-Dependent-Plasticity-Finds-the-Start-Masquelier-Guyonneau/432b5bfa6fc260289fef45544a43ebcd8892915e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports will be used in the notebook\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIF neuron model\n",
    "\n",
    "The LIF neuron model used in this experiment is based on Gerstner's [Spike Response Model](http://lcn.epfl.ch/~gerstner/SPNM/node26.html#SECTION02311000000000000000).\n",
    "\n",
    "At every time-step, the neuron membrane potential p is given by the formula:\n",
    "\n",
    "$$p=\\eta(t-t_{i})\\sum_{j|t_{j}>t_{i}}{}w_{j}\\varepsilon(t-t_{j})$$\n",
    "\n",
    "where $\\eta(t-t_{i})$ is the membrane response after a spike at time $t_{i}$:\n",
    "\n",
    "$$\\eta(t-t_{i})=K_{1}exp(-\\frac{t-t_{i}}{\\tau_{m}})-K_{2}(exp(-\\frac{t-t_{i}}{\\tau_{m}})-exp(-\\frac{t-t_{i}}{\\tau_{s}}))$$\n",
    "\n",
    "and $\\varepsilon(t)$ describes the Excitatory Post-Synaptic Potential of each synapse spike at time $t_{j}$:\n",
    "\n",
    "$$\\varepsilon(t-t_{j})=K(exp(-\\frac{t-t_{j}}{\\tau_{m}})-exp(-\\frac{t-t_{j}}{\\tau_{s}}))$$\n",
    "\n",
    "Note that K has to be chosen so that the max of $\\eta(t)$ is 1, knowing that $\\eta(t)$ is maximum when:\n",
    "$$t=\\frac{\\tau_{m}\\tau_{s}}{\\tau_{m}-\\tau_{s}}ln(\\frac{\\tau_{m}}{\\tau_{s}})$$\n",
    "\n",
    "In this simplified version of the neuron, the synaptic weights $w_{j}$ remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=None,\n",
    "                 K=2.1, K1=2.0, K2=4.0):\n",
    "\n",
    "        # Model parameters\n",
    "\n",
    "        # Membrane resting potential\n",
    "        self.p_rest = p_rest\n",
    "        \n",
    "        # Duration of the recovery period\n",
    "        self.tau_rest = tau_rest\n",
    "        \n",
    "        # Membrane time constant\n",
    "        self.tau_m = tau_m\n",
    "        \n",
    "        # Synaptic time constant\n",
    "        self.tau_s = tau_s\n",
    "        \n",
    "        # Spiking threshold\n",
    "        if T is None:\n",
    "            self.T = n_syn/4\n",
    "        else:\n",
    "            self.T = T\n",
    "        \n",
    "        # Model constants\n",
    "        self.K = K\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "\n",
    "        # The number of synapses\n",
    "        self.n_syn = n_syn\n",
    "        \n",
    "        # The synapse efficacy weights\n",
    "        self.w = tf.Variable(W)\n",
    "        \n",
    "        # The incoming spike times memory window\n",
    "        if max_spikes is None:\n",
    "            self.max_spikes = 70\n",
    "        else:\n",
    "            self.max_spikes = max_spikes\n",
    "\n",
    "        # Placeholders (ie things that are fed to the graph at runtime)\n",
    "\n",
    "        # A boolean tensor indicating which synapses have spiked during dt\n",
    "        self.new_spikes = tf.placeholder(shape=[self.n_syn], dtype=tf.bool, name='new_spikes')\n",
    "\n",
    "        # The time increment since the last update\n",
    "        self.dt = tf.placeholder(dtype=tf.float32, name='dt')\n",
    "        \n",
    "        # Variables (ie things that are modified by the graph at runtime)\n",
    "\n",
    "        # The neuron memory of incoming spike times\n",
    "        self.t_spikes = tf.Variable(tf.constant(100000.0, shape=[self.max_spikes, self.n_syn]), dtype=tf.float32)\n",
    "        \n",
    "        # The last spike time insertion index\n",
    "        self.t_spikes_idx = tf.Variable(self.n_syn - 1, dtype=tf.int32)\n",
    "\n",
    "        # The relative time since the last spike (assume it was a very long time ago)\n",
    "        self.last_spike = tf.Variable(1000.0, dtype=tf.float32, name='last_spike')\n",
    "        \n",
    "        # The membrane potential\n",
    "        self.p = tf.Variable(self.p_rest,dtype=tf.float32, name='p')\n",
    "        \n",
    "        # The duration remaining in the resting period (between 0 and self.tau_s)\n",
    "        self.t_rest = tf.Variable(0.0,dtype=tf.float32, name='t_rest')\n",
    "\n",
    "    # Excitatory post-synaptic potential (EPSP)\n",
    "    def epsilon_op(self):\n",
    "\n",
    "        # We only use the negative value of the relative spike times\n",
    "        spikes_t_op = tf.negative(self.t_spikes)\n",
    "\n",
    "        return self.K *(tf.exp(spikes_t_op/self.tau_m) - tf.exp(spikes_t_op/self.tau_s))\n",
    "    \n",
    "    # Membrane spike response\n",
    "    def eta_op(self):\n",
    "        \n",
    "        # We only use the negative value of the relative time\n",
    "        t_op = tf.negative(self.last_spike)\n",
    "        \n",
    "        # Evaluate the spiking positive pulse\n",
    "        pos_pulse_op = self.K1 * tf.exp(t_op/self.tau_m)\n",
    "        \n",
    "        # Evaluate the negative spike after-potential\n",
    "        neg_after_op = self.K2 * (tf.exp(t_op/self.tau_m) - tf.exp(t_op/self.tau_s))\n",
    "\n",
    "        # Evaluate the new post synaptic membrane potential\n",
    "        return self.T * (pos_pulse_op - neg_after_op)\n",
    "    \n",
    "    # Neuron behaviour during integrating phase (t_rest = 0)\n",
    "    def w_epsilons_op(self):\n",
    "        \n",
    "        # Evaluate synaptic EPSPs. We ignore synaptic spikes older than the last neuron spike\n",
    "        epsilons_op = tf.where(tf.logical_and(self.t_spikes >=0, self.t_spikes < self.last_spike - self.tau_rest),\n",
    "                               self.epsilon_op(),\n",
    "                               self.t_spikes*0.0)\n",
    "                          \n",
    "        # Agregate weighted incoming EPSPs \n",
    "        return tf.reduce_sum(self.w * epsilons_op)\n",
    "\n",
    "    # Neuron behaviour during resting phase (t_rest > 0)\n",
    "    def post_firing_p_op(self):\n",
    "   \n",
    "        # Membrane potential is only impacted by the last post-synaptic spike (ignore EPSPs)\n",
    "        return self.eta_op()\n",
    "    \n",
    "    def update_spikes_times(self):\n",
    "        \n",
    "        # Increase the age of all the existing spikes by dt\n",
    "        old_spikes_op = self.t_spikes.assign_add(tf.ones(tf.shape(self.t_spikes), dtype=tf.float32) * self.dt)\n",
    "\n",
    "        # Increment last spike index (modulo max_spikes)\n",
    "        new_idx_op = self.t_spikes_idx.assign(tf.mod(self.t_spikes_idx + 1, self.max_spikes))\n",
    "\n",
    "        # Create a list of coordinates to insert the new spikes\n",
    "        idx_op = tf.constant(1, shape=[self.n_syn], dtype=tf.int32) * new_idx_op\n",
    "        coord_op = tf.stack([idx_op, tf.range(self.n_syn)], axis=1)\n",
    "\n",
    "        # Create a vector of new spike times (non-spikes are assigned a very high time)\n",
    "        new_spikes_op = tf.where(self.new_spikes,\n",
    "                                 tf.constant(0.0, shape=[self.n_syn]),\n",
    "                                 tf.constant(100000.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Replace older spikes by new ones\n",
    "        return tf.scatter_nd_update(old_spikes_op, coord_op, new_spikes_op)\n",
    "    \n",
    "    def resting_w_op(self):\n",
    "        \n",
    "        # For the base LIF Neuron, the weights remain constants when resting\n",
    "        return tf.identity(self.w)\n",
    "    \n",
    "    def default_w_op(self):\n",
    "        \n",
    "        # For the base LIF Neuron, the weights remain constants when integrating\n",
    "        return tf.identity(self.w)\n",
    "\n",
    "    def firing_w_op(self):\n",
    "\n",
    "        # For the base LIF Neuron, the weights remain constants when firing\n",
    "        return tf.identity(self.w)\n",
    "    \n",
    "    def resting_op(self):\n",
    "        \n",
    "        # Update weights\n",
    "        w_op = self.resting_w_op()\n",
    "        \n",
    "        # Update the resting period\n",
    "        t_rest_op = self.t_rest.assign(tf.maximum(self.t_rest - self.dt, 0.0))\n",
    "        \n",
    "        # During the resting period, the membrane potential is only given by the eta kernel\n",
    "        with tf.control_dependencies([w_op, t_rest_op]):\n",
    "            return self.eta_op()\n",
    "    \n",
    "    def firing_op(self):\n",
    "\n",
    "        # Update weights\n",
    "        w_op = self.firing_w_op()\n",
    "        \n",
    "        # Reset the time of the last spike, but only once the weights have been updated\n",
    "        with tf.control_dependencies([w_op]):\n",
    "            last_spike_op = self.last_spike.assign(0.0)\n",
    "\n",
    "        # Start the resting period\n",
    "        t_rest_op = self.t_rest.assign(self.tau_rest)\n",
    "        \n",
    "        # At spiking time, the membrane potential is only given by the eta kernel\n",
    "        with tf.control_dependencies([last_spike_op, t_rest_op]):\n",
    "            return self.eta_op()\n",
    "        \n",
    "    def default_op(self):\n",
    "        \n",
    "        # Update weights\n",
    "        w_op = self.default_w_op()\n",
    "        \n",
    "        # By default, the membrane potential is given by the sum of the eta kernel and the weighted epsilons\n",
    "        with tf.control_dependencies([w_op]):\n",
    "            return self.eta_op() + self.w_epsilons_op()\n",
    "        \n",
    "    def integrating_op(self):\n",
    "\n",
    "        # Evaluate the new membrane potential, integrating both synaptic input and spike dynamics\n",
    "        p_op = self.eta_op() + self.w_epsilons_op()\n",
    "\n",
    "        # We have a different behavior if we reached the threshold\n",
    "        return tf.cond(p_op > self.T,\n",
    "                       self.firing_op,\n",
    "                       self.default_op)\n",
    "    \n",
    "    def response(self):\n",
    "        \n",
    "        # Update our internal memory of the synapse spikes (age older spikes, add new ones)\n",
    "        update_spikes_op = self.update_spikes_times()\n",
    "        \n",
    "        # Increase the relative time of the last spike by the time elapsed\n",
    "        last_spike_age_op = self.last_spike.assign_add(self.dt)\n",
    "        \n",
    "        # Update the internal state of the neuron\n",
    "        with tf.control_dependencies([update_spikes_op, last_spike_age_op]):\n",
    "            return tf.cond(self.t_rest > 0.0,\n",
    "                           self.resting_op,\n",
    "                           self.integrating_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulate neuron with predefined synapse input\n",
    "\n",
    "We replicate the figure 3 of the original paper by stimulating a LIF neuron with six consecutive spikes.\n",
    "\n",
    "The neuron has a refractory period of 1 ms and a threshold of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test neuron response with constant synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 80\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "n_syn = 1\n",
    "# Spiking times\n",
    "spikes = [2.0, 23.0, 44.0, 45.0, 48.0, 61.0, 70.0]\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((n_syn), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = LIFNeuron(n_syn, W, T=1)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        syn_has_spiked = [t in spikes]\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "for spike  in spikes:\n",
    "    plt.axvline(x=spike, color='gray', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the original paper. we see that because of the leaky nature of the neuron, the stimulating spikes have to be nearly synchronous for the threshold to be reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Poisson spike trains with varying rate\n",
    "\n",
    "The original paper uses Poisson spike trains with a rate varying in the [0, 90] Hz interval, with a variation speed that itself varies in the [-1800, 1800] Hz interval (in random uniform increments in the [-360,360] interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that generates random spike trains\n",
    "class SpikeTrains(object):\n",
    "    \n",
    "    def __init__(self, n_syn, r_min=0.0, r_max=90.0, r=None, s_max=1800, ds_max=360, s=None, auto_vrate=True, delta_max=0):\n",
    "        \n",
    "        # Number of synapses\n",
    "        self.n_syn = n_syn\n",
    "        # Minimum and maximum spiking rate (in Hz)\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        # Spiking rate for each synapse (in Hz)\n",
    "        if r is None:\n",
    "            self.r = np.random.uniform(self.r_min, self.r_max, size=(n_syn))\n",
    "        else:\n",
    "            self.r = r\n",
    "        # Rate variation parameters\n",
    "        self.s_max = s_max\n",
    "        self.ds_max = ds_max\n",
    "        # Rate variation\n",
    "        if s is None:\n",
    "            self.s = np.random.uniform(-self.s_max, self.s_max, size=(self.n_syn))\n",
    "        else:\n",
    "            self.s = s\n",
    "        # Automatically apply rate variation when\n",
    "        self.auto_vrate = auto_vrate\n",
    "        # Maximum time between two spikes on each synapse (0 means no maximum) in ms\n",
    "        self.delta_max = delta_max\n",
    "\n",
    "        # Memory of spikes\n",
    "        self.spikes = None\n",
    "    \n",
    "    # Generate new spikes for the specified time interval (in ms)\n",
    "    # The new spikes are added to the existing spike trains.\n",
    "    # The method returns only the new set of spikes\n",
    "    def add_spikes(self, t):\n",
    "        \n",
    "        for step in range(t):\n",
    "            # Draw a random number for each synapse\n",
    "            x = np.random.uniform(0,1, size=(self.n_syn))\n",
    "            # Each synapse spikes if the drawn number is lower than the probablity\n",
    "            # given by the integration of the rate over one millisecond\n",
    "            spikes = x < self.r * 1e-3\n",
    "            # Keep a memory of our spikes\n",
    "            if self.spikes is None:\n",
    "                self.spikes = np.array([spikes])\n",
    "            else:\n",
    "                if self.delta_max > 0:\n",
    "                    # We force each synapse to spike at least every delta_max ms\n",
    "                    if self.spikes.shape[0] >= self.delta_max - 1:\n",
    "                        # Get the last delta_max -1 spike trains\n",
    "                        last_spikes = self.spikes[-(self.delta_max - 1):,:]\n",
    "                        # For each synapse, count non-zero items\n",
    "                        n_spikes = np.count_nonzero(last_spikes, axis=0)\n",
    "                        # Modify spikes to force a spike on synapses where the spike count is zero\n",
    "                        spikes = np.where(n_spikes > 0, spikes, True)\n",
    "                # Store spikes\n",
    "                self.spikes = np.append(self.spikes, [spikes], axis=0)\n",
    "            if self.auto_vrate:\n",
    "                self.change_rate()\n",
    "\n",
    "        return self.spikes[-t:,:]\n",
    "    \n",
    "    # Format a list of spike indexes\n",
    "    def get_spikes(self):\n",
    "        \n",
    "        real_spikes = np.argwhere(self.spikes > 0)\n",
    "        # We prefer having spikes in the range [1..n_syn]\n",
    "        spike_index = real_spikes[:,1] + 1\n",
    "        spike_timings = real_spikes[:,0]\n",
    "        \n",
    "        return spike_timings, spike_index\n",
    "    \n",
    "    # Change rate, applying the specified delta in Hz\n",
    "    def change_rate(self, delta=None):\n",
    "\n",
    "        # Update spiking rate\n",
    "        if delta is None:\n",
    "            delta = self.s\n",
    "        self.r = np.clip( self.r + delta, self.r_min, self.r_max)\n",
    "        # Update spiking rate variation\n",
    "        ds = np.random.uniform(-self.ds_max, self.ds_max, size=(self.n_syn))\n",
    "        self.s = np.clip( self.s + ds, -self.s_max, self.s_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulate a LIF Neuron with random spike trains\n",
    "\n",
    "We now feed the neuron with 500 synapses that generate spikes at random interval with varying rates.\n",
    "\n",
    "The synaptic efficacy weights are arbitrarily set to 0.475 and remain constant throughout the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with random spike trains and constant synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 3000\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "n_syn = 500\n",
    "# Our random spike trains\n",
    "spike_trains = SpikeTrains(n_syn)\n",
    "# Generate spikes over the specified period\n",
    "syn_has_spiked = spike_trains.add_spikes(T)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((n_syn), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = LIFNeuron(n_syn, W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        feed = { neuron.new_spikes: syn_has_spiked[step], neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw the neuron membrane response to the 500 random synaptic spike trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(15,6)\n",
    "# Draw input spikes\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, spike_trains.n_syn])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('synapses')\n",
    "plt.xlabel('Time (msec)')\n",
    "t, spikes = spike_trains.get_spikes()\n",
    "plt.scatter(t, spikes, s=2)\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the neuron mostly saturates and continuously generates spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Spike Timing Dependent Plasticity\n",
    "\n",
    "We extend the LIFNeuron by allowing it to modify its synapse weights using a Spike Timing Dependent Plasticity algorithm.\n",
    "\n",
    "The STDP algorithm rewards synapses where spikes occurred immediately before a neuron spike, and inflicts penalties to the synapses where spikes occur after the neuron spike.\n",
    "\n",
    "The 'rewards' are called Long Term synaptic Potentiation (LTP), and the penalties Long Term synaptic Depression (LTD).\n",
    "\n",
    "For each synapse that spiked $\\Delta{t}$ before a neuron spike:\n",
    "\n",
    "$$\\Delta{w} = a^{+}exp(-\\frac{\\Delta{t}}{\\tau^{+}})$$\n",
    "\n",
    "For each synapse that spikes $\\Delta{t}$ after a neuron spike:\n",
    "\n",
    "$$\\Delta{w} = -a^{-}exp(-\\frac{\\Delta{t}}{\\tau^{-}})$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDPLIFNeuron(LIFNeuron):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=None,\n",
    "                 K=2.1, K1=2.0, K2=4.0,\n",
    "                 a_plus=0.03125, a_minus=0.0265625, tau_plus=16.8, tau_minus=33.7):\n",
    "        \n",
    "        # Call the parent contructor\n",
    "        super(STDPLIFNeuron, self).__init__(n_syn, W, max_spikes,\n",
    "                                            p_rest, tau_rest, tau_m, tau_s, T,\n",
    "                                            K, K1, K2)\n",
    "        \n",
    "        self.a_plus = a_plus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.a_minus = a_minus\n",
    "        self.tau_minus = tau_minus\n",
    "    \n",
    "    # Long Term synaptic Potentiation\n",
    "    def LTP_op(self):\n",
    "        \n",
    "        # Reward all spikes in our memory that happened before the new spike, but after the previous one\n",
    "        rewards_op = tf.where(self.t_spikes < self.last_spike,\n",
    "                              tf.constant(self.a_plus, shape=[self.max_spikes, self.n_syn]) * tf.exp(tf.negative(self.t_spikes/self.tau_plus)),\n",
    "                              tf.constant(0.0, shape=[self.max_spikes, self.n_syn]))                              \n",
    "        \n",
    "        # Accumulate rewards for each synapse along the history axis\n",
    "        acc_rewards_op = tf.reduce_sum(rewards_op,0)\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.add(self.w, acc_rewards_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "    \n",
    "    # Long Term synaptic Depression\n",
    "    def LTD_op(self):\n",
    "\n",
    "        # Gather all spikes corresponding to the last insertion index\n",
    "        new_spikes_op = tf.gather(self.t_spikes, self.t_spikes_idx)\n",
    "\n",
    "        # Inflict penalties, inversely exponential to the time since the last spike\n",
    "        penalties_op = tf.where(new_spikes_op <= 0.0, # Older spikes at this index have positive times\n",
    "                                tf.constant(self.a_minus, shape=[self.n_syn]) * tf.exp(tf.negative(self.last_spike/self.tau_minus)),\n",
    "                                tf.constant(0.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.subtract(self.w, penalties_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "\n",
    "    def firing_w_op(self):\n",
    "        \n",
    "        return self.LTP_op()\n",
    "\n",
    "    def default_w_op(self):\n",
    "        \n",
    "        # Apply long-term synaptic depression if we are still close to the last spike\n",
    "        # Note that if we unconditionally applied the LTD, the weights will slowly\n",
    "        # decrease to zero if no spike occurs.\n",
    "        return tf.cond(self.last_spike < self.tau_minus*7,\n",
    "                       self.LTD_op,\n",
    "                       lambda: tf.identity(self.w))\n",
    "    \n",
    "    def resting_w_op(self):\n",
    "        \n",
    "        return self.default_w_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test STDP with predefined input\n",
    "\n",
    "We apply the same predefined spike train to an STDP capable LIFNeuron with a limited number of synapses, and draw the resulting rewards (green) and penalties (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STDP algorithm with predefined input\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 80\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 5\n",
    "# Construct an array of spike inputs, initially empty\n",
    "spikes = np.zeros((steps,m), dtype=np.bool)\n",
    "# Add a few spikes\n",
    "spike_times = np.array([2, 23, 44, 45, 48, 61, 70])\n",
    "spike_index = np.array([1,  3,  2,  0,  4,  1,  3])\n",
    "spikes[spike_times, spike_index] = True\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    w_prev = W\n",
    "    delta_weights = np.zeros((steps, m))\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        feed = { neuron.new_spikes: spikes[step,:], neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))\n",
    "        w_next = neuron.w.eval()\n",
    "        delta_weights[step,:] = w_next - w_prev\n",
    "        w_prev = w_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(6,3)\n",
    "# Draw input spikes, penalties and rewards\n",
    "rewards = np.argwhere(delta_weights > 0)\n",
    "rewards_timings = rewards[:,0]\n",
    "rewards_index = rewards[:,1] + 1\n",
    "penalties = np.argwhere(delta_weights < 0)\n",
    "penalties_timings = penalties[:,0]\n",
    "penalties_index = penalties[:,1] + 1\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, m + 1])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('synapses')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_times, spike_index+1, s=100)\n",
    "for spike in spike_times:\n",
    "    plt.axvline(x=spike, color='gray', linestyle='--')\n",
    "plt.scatter(rewards_timings, rewards_index, color='lightgreen')\n",
    "plt.scatter(penalties_timings, penalties_index, color='red')\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "for spike in spike_times:\n",
    "    plt.axvline(x=spike, color='gray', linestyle='--')\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.plot(*zip(*P))\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the graph above, we verify that the rewards (green dots) are assigned only when the neuron spikes, and that they are assigned to synapses where a spike occured before the neuron spike (big blue dots).\n",
    "\n",
    "Note: a reward is assigned event if the synapse spike is not synchronous with the neuron spike, but it will be lower.\n",
    "\n",
    "We also verify that a penaly (red dot) is inflicted on every synapse where a spike occurs after a neuron spike.\n",
    "\n",
    "Note: these penalties may later be counter-balanced by a reward if a neuron spike closely follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulate an STDP LIF Neuron with random spike trains\n",
    "\n",
    "The goal here is to check the effects of the STDP learning on the neuron behaviour when it is stimulated with our random spike trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulate an STDP LIF neuron with random input at varying rates\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 3000\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "n_syn = 500\n",
    "# Spike trains\n",
    "spike_trains = SpikeTrains(n_syn)\n",
    "# Generate spikes over the specified period\n",
    "syn_has_spiked = spike_trains.add_spikes(T)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((n_syn), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(n_syn, W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        feed = { neuron.new_spikes: syn_has_spiked[step], neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(15,6)\n",
    "# Draw input spikes\n",
    "spike_timings, spike_index = spike_trains.get_spikes()\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, spike_trains.n_syn])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('synapses')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_timings, spike_index, s=1)\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consequence of this steady stimulation is a low decrease of the synaptic efficacy weights, down to the point where the neuron is not able to fire anymore.\n",
    "\n",
    "This is an adverse effect of the STDP algorithm used in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recurrent spike trains\n",
    "\n",
    "We don't follow exactly the same procedure as in the original paper, as the evolution of the hardware and software allows us to generate spike trains more easily. The result, however, is equivalent.\n",
    "\n",
    "We generate 2000 spike trains, from which we force the 1000 first to repeat a 50 ms pattern at random intervals.\n",
    "\n",
    "We first define a random 50ms sequence, that will be used as input when the pattern is played.\n",
    "\n",
    "We then generate random spike trains at every time-step: for the whole population if we are outside the pattern, for half of it otherwise.\n",
    "\n",
    "The time to the next pattern is chosen with a probability of 0.25 among the next slices of 50 ms (omitting the first one to avoid consecutive patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spike trains containing a recurrent pattern\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 15000\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "n_syn = 2000\n",
    "# Number of synapses involved in the pattern\n",
    "n_syn_pattern = int(n_syn/2)\n",
    "# Output variables\n",
    "pattern_t = []\n",
    "\n",
    "# First, instantiate our base spike trains:\n",
    "# - variable rate,\n",
    "# - one spike every 50ms at a minimum\n",
    "spike_trains = SpikeTrains(n_syn, delta_max=50)\n",
    "spike_trains.add_spikes(T)\n",
    "\n",
    "# Then instantiate another set of spike trains to represent\n",
    "# spontaneous activity (10 Hz constant rate)\n",
    "spike_trains_c = SpikeTrains(n_syn, r=np.full((n_syn), 10), auto_vrate=False)\n",
    "spike_trains_c.add_spikes(T)\n",
    "\n",
    "# Create an empty array to host the final spike trains\n",
    "syn_has_spiked = np.zeros((steps, n_syn), dtype=np.bool)\n",
    "\n",
    "# We choose the beginning of the pattern in the [25,75] ms interval\n",
    "pat_start_time = np.random.randint(25,75)\n",
    "pattern_t.append(pat_start_time)\n",
    "\n",
    "for step in range(steps):\n",
    "        \n",
    "    t = int(step * dt)\n",
    "\n",
    "    # Initialize our spike vector from our base spike trains\n",
    "    syn_has_spiked[step,:] = spike_trains.spikes[step,:]\n",
    "\n",
    "    # Test if we are in the pattern interval\n",
    "    if t >= pat_start_time and t < (pat_start_time + 50):\n",
    "        # We just copy the pattern\n",
    "        syn_has_spiked[step,:n_syn_pattern] = spike_trains.spikes[t - pat_start_time + pattern_t[0],:n_syn_pattern]\n",
    "    else:\n",
    "        # Evaluate the time of the next pattern presentation\n",
    "        if t >= pat_start_time + 100:\n",
    "            pat_start_time = t\n",
    "            # We have 1/4 chances of replaying the pattern for each chunk of 50 ms\n",
    "            r = np.random.uniform(0,1)\n",
    "            while (r >= 0.25):\n",
    "                pat_start_time += 50\n",
    "                r = np.random.uniform(0,1)\n",
    "            pattern_t.append(pat_start_time)\n",
    "    # Add spontaneous activity\n",
    "    syn_has_spiked[step,:] |= spike_trains_c.spikes[step,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the resulting synapse mean spiking rates, and some samples of the spike trains, identifying the pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(15,6)\n",
    "# Evaluate the mean firing rate of each synapse in Hz\n",
    "rates = np.count_nonzero(syn_has_spiked, axis=0)*1000.0/T\n",
    "r_max = np.max(rates)\n",
    "r_mean_a = np.mean(rates[:n_syn_pattern])\n",
    "r_mean_b = np.mean(rates[n_syn_pattern:])\n",
    "plt.figure()\n",
    "plt.title('Synapse mean firing rates')\n",
    "plt.plot(rates)\n",
    "plt.axhline(y=r_mean_a, xmax=0.5, color='g', linestyle='--')\n",
    "plt.text(0,r_max -10,'mean rate: %d' % r_mean_a, color='g')\n",
    "plt.axhline(y=r_mean_b, xmin=0.5, color='r', linestyle='--')\n",
    "plt.text(n_syn,r_max -10,'mean rate: %d' % r_mean_b, ha='right', color='r')\n",
    "intervals = ([0,299],[7200,7499], [14700, 14999])\n",
    "for interval in intervals:\n",
    "    it_pattern_t = np.array(pattern_t)\n",
    "    it_pattern_t = it_pattern_t[np.logical_and(it_pattern_t >=interval[0], it_pattern_t <=interval[1])]\n",
    "    # Draw input spikes, identifying the patterns\n",
    "    plt.figure()\n",
    "    it_spikes = syn_has_spiked[interval[0]:interval[1]]\n",
    "    it_real_spikes = np.argwhere(it_spikes)\n",
    "    for pat_t in it_pattern_t:\n",
    "        plt.fill_between((pat_t,pat_t+50,pat_t+50,pat_t),(0,0,n_syn_pattern,n_syn_pattern),facecolor='lightgray')\n",
    "    t, s = it_real_spikes.T\n",
    "    plt.scatter(interval[0] + t,s+1,s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the mean spiking rate is the same for both population of synapses (64 Hz = 54 Hz + 10 Hz).\n",
    "\n",
    "We nevertheless notice that the standard deviation is much higher for the synapses involved in the pattern. \n",
    "\n",
    "On the spike trains samples, one can identify the patterns, slightly modified by the 10 Hz spontaneous activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulate an STDP LIF neuron with recurrent spiking trains\n",
    "\n",
    "We perform a simulation on our STDP LIF neuron with the generated spike trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with recurrent pattern\n",
    "\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((n_syn), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(n_syn, W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    response = neuron.response()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = int(step * dt)\n",
    "\n",
    "        # Evaluate the neuron response\n",
    "        feed = { neuron.new_spikes: syn_has_spiked[step], neuron.dt: dt}\n",
    "        p = sess.run(response, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] =(15,3)\n",
    "intervals = ([0,1999],[7000,8999], [13000, 14999])\n",
    "for interval in intervals:\n",
    "    it_P = P[interval[0]:interval[1]]\n",
    "    it_pattern_t = np.array(pattern_t)\n",
    "    it_pattern_t = it_pattern_t[np.logical_and(it_pattern_t >=interval[0], it_pattern_t <=interval[1])]\n",
    "    # Draw membrane potential, identifying the patterns\n",
    "    plt.figure()\n",
    "    for pat_t in it_pattern_t:\n",
    "        plt.fill_between((pat_t,pat_t+50,pat_t+50,pat_t),(-neuron.T/2,-neuron.T/2,neuron.T*2,neuron.T*2),facecolor='lightgray')\n",
    "    plt.plot(*zip(*it_P))\n",
    "    plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "    plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "    plt.title('LIF response')\n",
    "    plt.ylabel('Membrane Potential (mV)')\n",
    "    plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our disappointment, the neuron quickly saturates and doesn't learn anything !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
