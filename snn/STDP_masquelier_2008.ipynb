{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STDP Finds the Start of Repeating Patterns in Continuous Spike Trains\n",
    "\n",
    "In this notebook we will reproduce the experiments described in [Masquelier & Thorpe (2008)](https://www.semanticscholar.org/paper/Spike-Timing-Dependent-Plasticity-Finds-the-Start-Masquelier-Guyonneau/432b5bfa6fc260289fef45544a43ebcd8892915e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports will be used in the notebook\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =(12,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIF neuron model\n",
    "\n",
    "The LIF neuron model used in this experiment is based on Gerstner's [Spike Response Model](http://lcn.epfl.ch/~gerstner/SPNM/node26.html#SECTION02311000000000000000).\n",
    "\n",
    "At every time-step, the neuron membrane potential p is given by the formula:\n",
    "\n",
    "$$p=\\eta(t-t_{i})\\sum_{j|t_{j}>t_{i}}{}w_{j}\\varepsilon(t-t_{j})$$\n",
    "\n",
    "where $\\eta(t-t_{i})$ is the membrane response after a spike at time $t_{i}$:\n",
    "\n",
    "$$\\eta(t-t_{i})=K_{1}exp(-\\frac{t-t_{i}}{\\tau_{m}})-K_{2}(exp(-\\frac{t-t_{i}}{\\tau_{m}})-exp(-\\frac{t-t_{i}}{\\tau_{s}}))$$\n",
    "\n",
    "and $\\varepsilon(t)$ describes the Excitatory Post-Synaptic Potential of each synapse spike at time $t_{j}$:\n",
    "\n",
    "$$\\varepsilon(t-t_{j})=K(exp(-\\frac{t-t_{j}}{\\tau_{m}})-exp(-\\frac{t-t_{j}}{\\tau_{s}}))$$\n",
    "\n",
    "Note that K has to be chosen so that the max of $\\eta(t)$ is 1, knowing that $\\eta(t)$ is maximum when:\n",
    "$$t=\\frac{\\tau_{m}\\tau_{s}}{\\tau_{m}-\\tau_{s}}ln(\\frac{\\tau_{m}}{\\tau_{s}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=500.0,\n",
    "                 K=2.1, K1=2.0, K2=4.0):\n",
    "\n",
    "        # Model parameters\n",
    "\n",
    "        # Membrane resting potential\n",
    "        self.p_rest = p_rest\n",
    "        \n",
    "        # Duration of the recovery period\n",
    "        self.tau_rest = tau_rest\n",
    "        \n",
    "        # Membrane time constant\n",
    "        self.tau_m = tau_m\n",
    "        \n",
    "        # Synaptic time constant\n",
    "        self.tau_s = tau_s\n",
    "        \n",
    "        # Spiking threshold\n",
    "        self.T = T\n",
    "        \n",
    "        # Model constants\n",
    "        self.K = K\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "\n",
    "        # The number of synapses\n",
    "        self.n_syn = n_syn\n",
    "        \n",
    "        # The synapse efficacy weights\n",
    "        self.w = tf.Variable(W)\n",
    "        \n",
    "        # The incoming spike times memory window\n",
    "        if max_spikes is None:\n",
    "            self.max_spikes = 70\n",
    "        else:\n",
    "            self.max_spikes = max_spikes\n",
    "\n",
    "        # Placeholders (ie things that are fed to the graph at runtime)\n",
    "\n",
    "        # A boolean tensor indicating which synapses have spiked during dt\n",
    "        self.new_spikes = tf.placeholder(shape=[m], dtype=tf.bool, name='new_spikes')\n",
    "\n",
    "        # The time increment since the last update\n",
    "        self.dt = tf.placeholder(dtype=tf.float32, name='dt')\n",
    "        \n",
    "        # Variables (ie things that are modified by the graph at runtime)\n",
    "\n",
    "        # The neuron memory of incoming spike times\n",
    "        self.t_spikes = tf.Variable(tf.constant(100000.0, shape=[self.max_spikes, self.n_syn]), dtype=tf.float32)\n",
    "        \n",
    "        # The last spike time insertion index\n",
    "        self.t_spikes_idx = tf.Variable(self.n_syn - 1, dtype=tf.int32)\n",
    "\n",
    "        # The relative time since the last spike (assume it was a very long time ago)\n",
    "        self.last_spike = tf.Variable(1000.0, dtype=tf.float32, name='last_spike')\n",
    "        \n",
    "        # The membrane potential\n",
    "        self.p = tf.Variable(self.p_rest,dtype=tf.float32, name='p')\n",
    "        \n",
    "        # The duration remaining in the resting period (between 0 and self.tau_s)\n",
    "        self.t_rest = tf.Variable(0.0,dtype=tf.float32, name='t_rest')\n",
    "\n",
    "    # Excitatory post-synaptic potential (EPSP)\n",
    "    def epsilon_op(self):\n",
    "\n",
    "        # We only use the negative value of the relative spike times\n",
    "        spikes_t_op = tf.negative(self.t_spikes)\n",
    "\n",
    "        return self.K *(tf.exp(spikes_t_op/self.tau_m) - tf.exp(spikes_t_op/self.tau_s))\n",
    "    \n",
    "    # Membrane spike response\n",
    "    def eta_op(self):\n",
    "        \n",
    "        # We only use the negative value of the relative time\n",
    "        t_op = tf.negative(self.last_spike)\n",
    "        \n",
    "        # Evaluate the spiking positive pulse\n",
    "        pos_pulse_op = self.K1 * tf.exp(t_op/self.tau_m)\n",
    "        \n",
    "        # Evaluate the negative spike after-potential\n",
    "        neg_after_op = self.K2 * (tf.exp(t_op/self.tau_m) - tf.exp(t_op/self.tau_s))\n",
    "\n",
    "        # Evaluate the new post synaptic membrane potential\n",
    "        return self.T * (pos_pulse_op - neg_after_op)\n",
    "\n",
    "    # Neuron behaviour during integrating phase (below threshold)\n",
    "    def integrating_op(self):\n",
    "\n",
    "        # Increase the relative time of the last spike by the time elapsed\n",
    "        last_spike_op = self.last_spike.assign_add(self.dt)\n",
    "        \n",
    "        # Evaluate synaptic EPSPs. We ignore synaptic spikes older than the last neuron spike\n",
    "        epsilons_op = tf.where(tf.logical_and(self.t_spikes >=0, self.t_spikes < last_spike_op),\n",
    "                               self.epsilon_op(),\n",
    "                               self.t_spikes*0.0)\n",
    "                          \n",
    "        # Update the membrane potential with spike membrane response and weighted incoming EPSPs \n",
    "        p_op = self.p.assign(self.eta_op() + tf.reduce_sum(self.w * epsilons_op))\n",
    "        \n",
    "        return p_op\n",
    "                          \n",
    "    # Neuron behaviour during firing phase (above threshold)\n",
    "    def firing_op(self):\n",
    "\n",
    "        # Refractory period starts now\n",
    "        t_rest_op = self.t_rest.assign(self.tau_rest)\n",
    "\n",
    "        # Reset last spike time\n",
    "        last_spike_op = tf.assign(self.last_spike, self.dt)\n",
    "        \n",
    "        # Explicitly mark operations not used by p_op as dependencies\n",
    "        with tf.control_dependencies([t_rest_op, last_spike_op]):\n",
    "            # Reset membrane potential\n",
    "            p_op = self.p.assign(self.eta_op())\n",
    "\n",
    "        return p_op\n",
    "\n",
    "    # Neuron behaviour during resting phase (t_rest > 0)\n",
    "    def resting_op(self):\n",
    "\n",
    "        # Refractory period is decreased by dt\n",
    "        t_rest_op = self.t_rest.assign_sub(self.dt)\n",
    "\n",
    "        # Increase the relative time of the last spike by the time elapsed\n",
    "        last_spike_op = self.last_spike.assign_add(self.dt)\n",
    "        \n",
    "        # Explicitly mark operations not used by p_op as dependencies\n",
    "        with tf.control_dependencies([t_rest_op, last_spike_op]):     \n",
    "            # Membrane potential is only impacted by the last post-synaptic spike (ignore EPSPs)\n",
    "            p_op = self.p.assign(self.eta_op())\n",
    "\n",
    "        return p_op\n",
    "    \n",
    "    def update_spikes_times(self):\n",
    "        \n",
    "        # Increase the age of all the existing spikes by dt\n",
    "        old_spikes_op = self.t_spikes.assign_add(tf.ones(tf.shape(self.t_spikes), dtype=tf.float32) * self.dt)\n",
    "\n",
    "        # Increment last spike index (modulo max_spikes)\n",
    "        new_idx_op = self.t_spikes_idx.assign(tf.mod(self.t_spikes_idx + 1, self.max_spikes))\n",
    "\n",
    "        # Create a list of coordinates to insert the new spikes\n",
    "        idx_op = tf.constant(1, shape=[self.n_syn], dtype=tf.int32) * new_idx_op\n",
    "        coord_op = tf.stack([idx_op, tf.range(self.n_syn)], axis=1)\n",
    "\n",
    "        # Create a vector of new spike times (non-spikes are assigned a very high time)\n",
    "        new_spikes_op = tf.where(self.new_spikes,\n",
    "                                 tf.constant(0.0, shape=[self.n_syn]),\n",
    "                                 tf.constant(100000.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Replace older spikes by new ones\n",
    "        return tf.scatter_nd_update(old_spikes_op, coord_op, new_spikes_op)\n",
    "\n",
    "    def update_op(self):\n",
    "        \n",
    "        update_spikes_op = self.update_spikes_times()\n",
    "        # Synapse spikes are only indirectly used in the integrating phase,\n",
    "        # so we need to force the dependency to have them updated every time\n",
    "        with tf.control_dependencies([update_spikes_op]):\n",
    "            return tf.case(\n",
    "                [\n",
    "                    (self.t_rest > 0.0, self.resting_op),\n",
    "                    (self.p > self.T, self.firing_op),\n",
    "                ],\n",
    "                default=self.integrating_op\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate neuron response with random input\n",
    "\n",
    "We feed the neuron with 2000 synapses that generate spikes at random interval with a frequency of 45 Hz.\n",
    "\n",
    "The synaptic efficacy weights are arbitrarily set to 0.475 and remain constant throughout the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with constant synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 2000\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We need to keep track of input spikes over time\n",
    "spikes = np.full((1,m), -1.0, dtype=np.float32)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = LIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    update_op = neuron.update_op()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        if spikes.size > 0:\n",
    "            # Increase all relative spike times by dt\n",
    "            # Non-spikes slots are identified by negative numbers\n",
    "            spikes[spikes >= 0] += dt\n",
    "\n",
    "        r = np.random.uniform(0,1, size=(m))\n",
    "        syn_has_spiked = r < f * dt\n",
    "        if np.count_nonzero(syn_has_spiked) > 0:\n",
    "            spikes = np.append(spikes,np.where(syn_has_spiked, 0.0, -1.0).reshape((1,m)), axis=0)\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(update_op, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw the neuron membrane response to the 2000 random synaptic spike trains. We can see that the neuron mostly saturates and continuously generate spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw input spikes\n",
    "real_spikes = np.argwhere(spikes >=0)\n",
    "spike_index = real_spikes[:,1] + 1\n",
    "spike_timings = spikes[spikes >=0]\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, m])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('spikes')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_timings, spike_index, s=2)\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDPLIFNeuron(LIFNeuron):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_syn, W, max_spikes=None, \n",
    "                 p_rest=0.0, tau_rest=1.0, tau_m=10.0, tau_s=2.5, T=500.0,\n",
    "                 K=2.1, K1=2.0, K2=4.0,\n",
    "                 a_plus=0.03125, a_minus=0.0265625, tau_plus=16.8, tau_minus=33.7):\n",
    "        \n",
    "        # Call the parent contructor\n",
    "        super(STDPLIFNeuron, self).__init__(n_syn, W, max_spikes,\n",
    "                                            p_rest, tau_rest, tau_m, tau_s, T,\n",
    "                                            K, K1, K2)\n",
    "        \n",
    "        self.a_plus = a_plus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.a_minus = a_minus\n",
    "        self.tau_minus = tau_minus\n",
    "    \n",
    "    # Long Term synaptic Potentiation\n",
    "    def LTP_op(self):\n",
    "        \n",
    "        # Reward all spikes in our memory that happened before the new spike\n",
    "        rewards_op = self.a_plus * tf.exp(tf.negative(self.t_spikes/self.tau_plus))\n",
    "        \n",
    "        # Accumulate rewards for each synapse along the history axis\n",
    "        acc_rewards_op = tf.reduce_sum(rewards_op,0)\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.add(self.w, acc_rewards_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "    \n",
    "    # Long Term synaptic Depression\n",
    "    def LTD_op(self):\n",
    "\n",
    "        # Gather all spikes corresponding to the last insertion index\n",
    "        new_spikes_op = tf.gather(self.t_spikes, self.t_spikes_idx)\n",
    "\n",
    "        # Inflict penalties, inversely exponential to the time since the last spike\n",
    "        penalties_op = tf.where(new_spikes_op <= 0.0, # Older spikes at this index have positive times\n",
    "                                tf.constant(self.a_minus, shape=[self.n_syn]) * tf.exp(tf.negative(self.last_spike/self.tau_minus)),\n",
    "                                tf.constant(0.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Evaluate new weights\n",
    "        new_w_op = tf.subtract(self.w, penalties_op)\n",
    "        \n",
    "        # Update with new weights clamped to [0,1]\n",
    "        return self.w.assign(tf.clip_by_value(new_w_op, 0.0, 1.0))\n",
    "    \n",
    "    def stdp_firing_op(self):\n",
    "        \n",
    "        # Apply long-term synaptic potentiation\n",
    "        ltp_op = self.LTP_op()\n",
    "        \n",
    "        # Refractory period starts now\n",
    "        t_rest_op = self.t_rest.assign(self.tau_rest)\n",
    "\n",
    "        # Reset last spike time\n",
    "        last_spike_op = tf.assign(self.last_spike, self.dt)\n",
    "        \n",
    "        # Explicitly mark operations not used by p_op as dependencies\n",
    "        with tf.control_dependencies([ltp_op, t_rest_op, last_spike_op]):\n",
    "            # Reset membrane potential\n",
    "            p_op = self.p.assign(self.eta_op())\n",
    "\n",
    "            return p_op\n",
    "\n",
    "    def update_op(self):\n",
    "        \n",
    "        # Update spike times\n",
    "        update_spikes_op = self.update_spikes_times()\n",
    "        \n",
    "        with tf.control_dependencies([update_spikes_op]):\n",
    "            # Apply long-term synaptic depression if we are still close to the last spike\n",
    "            # Note that if we unconditionally applied the LTD, the weights will slowly\n",
    "            # decrease to zero if no spike occurs.\n",
    "            ltd_op = tf.cond(self.last_spike < self.tau_minus*7, self.LTD_op, lambda: tf.identity(self.w))\n",
    "            with tf.control_dependencies([ltd_op]):\n",
    "                return tf.case(\n",
    "                    [\n",
    "                        (self.t_rest > 0.0, self.resting_op),\n",
    "                        (self.p > self.T, self.stdp_firing_op),\n",
    "                    ],\n",
    "                    default=self.integrating_op\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with evolving synaptic weights\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 500\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 2000\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We need to keep track of input spikes over time\n",
    "spikes = np.full((1,m), -1.0, dtype=np.float32)\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    update_op = neuron.update_op()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        if spikes.size > 0:\n",
    "            # Increase all relative spike times by dt\n",
    "            # Non-spikes slots are identified by negative numbers\n",
    "            spikes[spikes >= 0] += dt\n",
    "\n",
    "        r = np.random.uniform(0,1, size=(m))\n",
    "        syn_has_spiked = r < f * dt\n",
    "        if np.count_nonzero(syn_has_spiked) > 0:\n",
    "            spikes = np.append(spikes,np.where(syn_has_spiked, 0.0, -1.0).reshape((1,m)), axis=0)\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(update_op, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw input spikes\n",
    "real_spikes = np.argwhere(spikes >=0)\n",
    "spike_index = real_spikes[:,1] + 1\n",
    "spike_timings = spikes[spikes >=0]\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, m])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('spikes')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(spike_timings, spike_index, s=1)\n",
    "# Draw membrane potential\n",
    "plt.figure()\n",
    "plt.plot(*zip(*P))\n",
    "plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recurrent spike trains\n",
    "\n",
    "We don't follow exactly the same procedure as in the original paper, as the evolution of the hardware and software allows us to generate spike trains more easily. The result, however, is equivalent.\n",
    "\n",
    "We generate 2000 spike trains, from which we force the 1000 first to repeat a 50 ms pattern at random intervals.\n",
    "\n",
    "We first define a random 50ms sequence, that will be used as input when the pattern is played.\n",
    "\n",
    "We then generate random spike trains at every time-step: for the whole population if we are outside the pattern, for half of it otherwise.\n",
    "\n",
    "The time to the next pattern is chosen with a probability of 0.25 among the next slices of 50 ms (omitting the first one to avoid consecutive patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with recurrent pattern\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 15000\n",
    "# Duration of each time step in ms\n",
    "dt = 1.0\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "m = 2000\n",
    "# Spiking frequency in Hz\n",
    "f = 4.5e-2\n",
    "# We define the base synaptic efficacy as a uniform vector\n",
    "W = np.full((m), 0.475, dtype=np.float32)\n",
    "# Output variables\n",
    "P = []\n",
    "pattern_t = []\n",
    "spike_trains = np.zeros((T,m), dtype=np.bool)\n",
    "\n",
    "# First, create the 50 ms pattern\n",
    "n_syn_pattern = int(m/2)\n",
    "pattern = np.zeros((50, n_syn_pattern))\n",
    "for step in range(50):\n",
    "\n",
    "    r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "    pattern[step,:] = r < f\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    neuron = STDPLIFNeuron(m,W)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    update_op = neuron.update_op()\n",
    "    \n",
    "    syn_has_spiked = np.zeros((m), dtype=np.bool)\n",
    "\n",
    "    pat_start_time = np.random.randint(25,75)\n",
    "    pattern_t.append(pat_start_time)\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = int(step * dt)\n",
    "\n",
    "        # Evaluate the first population of neuron behavior\n",
    "        if t >= pat_start_time and t < (pat_start_time + 50):\n",
    "            # We just copy the pattern\n",
    "            syn_has_spiked[:n_syn_pattern] = pattern[t - pat_start_time]\n",
    "        else:\n",
    "            # Generate new random spikes\n",
    "            r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "            syn_has_spiked[:n_syn_pattern] = r < f * dt\n",
    "            # Evaluate the time of the next pattern\n",
    "            if t >= pat_start_time + 100:\n",
    "                pat_start_time = t\n",
    "                # We have 1/4 chances of replaying the pattern for each chunk of 50 ms\n",
    "                r = np.random.uniform(0,1)\n",
    "                while (r >= 0.25):\n",
    "                    pat_start_time += 50\n",
    "                    r = np.random.uniform(0,1)\n",
    "                pattern_t.append(pat_start_time)\n",
    "        # Evaluate the second population of neuron behavior                \n",
    "        r = np.random.uniform(0,1, size=(n_syn_pattern))\n",
    "        syn_has_spiked[n_syn_pattern:] = r < f * dt\n",
    "        spike_trains[step,:] = syn_has_spiked\n",
    "        feed = { neuron.new_spikes: syn_has_spiked, neuron.dt: dt}\n",
    "        p = sess.run(update_op, feed_dict=feed)\n",
    "        P.append((t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = ([0,500],[7500, 7999],[14500,14999])\n",
    "for interval in intervals:\n",
    "    it_spike_trains = spike_trains[interval[0]:interval[1],:]\n",
    "    it_P = P[interval[0]:interval[1]]\n",
    "    it_pattern_t = np.array(pattern_t)\n",
    "    it_pattern_t = it_pattern_t[np.logical_and(it_pattern_t >=interval[0], it_pattern_t <=interval[1])]\n",
    "    # Draw input spikes, identifying the patterns\n",
    "    spikes = np.argwhere(it_spike_trains == True)\n",
    "    plt.figure()\n",
    "    plt.axis([interval[0], interval[1], 0, m])\n",
    "    plt.title('Synaptic spikes')\n",
    "    plt.ylabel('spikes')\n",
    "    plt.xlabel('Time (msec)')\n",
    "    for pat_t in it_pattern_t:\n",
    "        plt.fill_between((pat_t,pat_t+50,pat_t+50,pat_t),(0,0,m/2,m/2),facecolor='gray')\n",
    "    t, s = spikes.T\n",
    "    plt.scatter(t+interval[0], s, s=1)\n",
    "    # Draw membrane potential\n",
    "    plt.figure()\n",
    "    plt.plot(*zip(*it_P))\n",
    "    plt.axhline(y=neuron.T, color='r', linestyle='-')\n",
    "    plt.axhline(y=neuron.p_rest, color='y', linestyle='--')\n",
    "    plt.title('LIF response')\n",
    "    plt.ylabel('Membrane Potential (mV)')\n",
    "    plt.xlabel('Time (msec)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
